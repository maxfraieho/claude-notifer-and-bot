#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
üöÄ –†–æ–∑—à–∏—Ä–µ–Ω–∏–π –ê—É–¥–∏—Ç–æ—Ä –õ–æ–≥—ñ–∫–∏ Telegram –ë–æ—Ç–∞ (Claude Code)
–§–æ–∫—É—Å: –†–µ–∞–ª—å–Ω—ñ –ø—Ä–æ–±–ª–µ–º–∏ –¥–æ—Å–≤—ñ–¥—É –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞ (User Experience), –æ—Å–æ–±–ª–∏–≤–æ –¥–ª—è —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—ó –ª–æ–∫–∞–ª—ñ–∑–∞—Ü—ñ—ó

–ê–≤—Ç–æ—Ä: AI –ê—Å–∏—Å—Ç–µ–Ω—Ç
–ú–æ–≤–∞ –∑–≤—ñ—Ç—ñ–≤: –£–∫—Ä–∞—ó–Ω—Å—å–∫–∞
–í–µ—Ä—Å—ñ—è: 3.0
"""

import os
import re
import ast
import json
import logging
from pathlib import Path
from typing import Dict, List, Set, Tuple, Optional, Any, Callable
from datetime import datetime
import sys

# –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –ª–æ–≥—É–≤–∞–Ω–Ω—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class AdvancedBotAuditor:
    """–ì–æ–ª–æ–≤–Ω–∏–π –∫–ª–∞—Å –∞—É–¥–∏—Ç–æ—Ä–∞, —è–∫–∏–π –∞–Ω–∞–ª—ñ–∑—É—î –±–æ—Ç –Ω–∞ —Ä–µ–∞–ª—å–Ω—ñ –ø—Ä–æ–±–ª–µ–º–∏ UX."""

    def __init__(self, source_dir: str = "src", report_lang: str = "uk"):
        """
        –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –∞—É–¥–∏—Ç–æ—Ä–∞.

        :param source_dir: –®–ª—è—Ö –¥–æ –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—ó –∑ –≤–∏—Ö—ñ–¥–Ω–∏–º –∫–æ–¥–æ–º (–∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º "src")
        :param report_lang: –ú–æ–≤–∞ –∑–≤—ñ—Ç—É ("uk" –∞–±–æ "en")
        """
        self.source_dir = Path(source_dir)
        if not self.source_dir.exists():
            raise FileNotFoundError(f"–î–∏—Ä–µ–∫—Ç–æ—Ä—ñ—é {source_dir} –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ")

        self.report_lang = report_lang
        self.findings = {
            'critical': [],
            'localization': [],
            'ux': [],
            'integration': [],
            'buttons': []
        }

        # –®–ª—è—Ö–∏ –¥–æ —Ñ–∞–π–ª—ñ–≤ –ø–µ—Ä–µ–∫–ª–∞–¥—ñ–≤
        self.translations = {}
        self.translation_files = {
            'en': self.source_dir / "localization" / "translations" / "en.json",
            'uk': self.source_dir / "localization" / "translations" / "uk.json"
        }

        # –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω—ñ –∫–ª—é—á—ñ –ø–µ—Ä–µ–∫–ª–∞–¥—ñ–≤
        self.translation_keys = {'en': set(), 'uk': set()}

        # –ü–∞—Ç–µ—Ä–Ω–∏ –¥–ª—è –≤–∏—è–≤–ª–µ–Ω–Ω—è –∫—Ä–∏—Ç–∏—á–Ω–∏—Ö –ø—Ä–æ–±–ª–µ–º
        self.CRITICAL_PATTERNS = {
            'dead_commands': [
                r'@register_command\(["\'](\w+)["\'].*?async def.*?raise NotImplementedError',
                r'CommandHandler\(["\'](\w+)["\'].*?pass\b',
                r'reply_text\([rf]?["\'][^"\']*Error[^"\']*["\'].*?# TODO',
                r'NotImplementedError'
            ],
            'silent_failures': [
                r'except\s*:\s*pass(?!\s*#)',
                r'except\s*:\s*continue(?!\s*#)',
                r'try:.*?except.*?:\s*return\s+None',
                r'try:.*?except.*?:\s*break'
            ],
            'user_facing_errors': [
                r'reply_text\([rf]?["\'][^"\']*(?:Exception|Error|Failed|Invalid|Timeout|Permission)[^"\']*["\']',
                r'await.*?reply.*?code\s*\d+',
                r'raise\s+\w+Error\(["\'].*?["\']\)'
            ],
            'broken_buttons': [
                r'InlineKeyboardButton\(["\']([^"\']+)["\'].*?callback_data=["\'](\w+)["\']'
            ]
        }

        # –ü–∞—Ç–µ—Ä–Ω–∏ –¥–ª—è –≤–∏—è–≤–ª–µ–Ω–Ω—è –ø—Ä–æ–±–ª–µ–º UX
        self.UX_PATTERNS = {
            'mixed_languages': [
                r'[–∞-—è—ñ—ó—î“ë–ê-–Ø–Ü–á–Ñ“ê]+.*?[a-zA-Z].*?reply_text',  # –£–∫—Ä–∞—ó–Ω—Å—å–∫–∏–π + –∞–Ω–≥–ª—ñ–π—Å—å–∫–∏–π —Ç–µ–∫—Å—Ç
                r'‚ùå.*?[A-Z][a-z]+.*?Error',  # –ê–Ω–≥–ª—ñ–π—Å—å–∫–∞ –ø–æ–º–∏–ª–∫–∞ –∑ —É–∫—Ä–∞—ó–Ω—Å—å–∫–∏–º –µ–º–æ–¥–∑—ñ
                r'‚ö†Ô∏è.*?[A-Z][a-z]+.*?Error',
                r'‚úÖ.*?[A-Z][a-z]+.*?Success'
            ],
            'poor_error_messages': [
                r'reply_text\(["\']‚ùå[^"\']*["\'].*?\)',  # –ó–∞–≥–∞–ª—å–Ω—ñ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è –ø–æ–º–∏–ª–æ–∫
                r'Exception.*?str\(e\)',  # –°–∏—Ä–∏–π —Ç–µ–∫—Å—Ç –≤–∏–∫–ª—é—á–µ–Ω–Ω—è
                r'raise\s+Exception\([\'"][^\'"]',
                r'logger\.error\([\'"][^\'"]'
            ],
            'hardcoded_strings': [
                r'reply_text\([rf]?["\']([^"\']{10,}[^"\']*)["\']',  # –î–æ–≤–≥—ñ —Ä—è–¥–∫–∏ –≤ reply_text
                r'send_message\([rf]?["\']([^"\']{10,}[^"\']*)["\']',
                r'answer\([rf]?["\']([^"\']{10,}[^"\']*)["\']',
                r'edit_message_text\([rf]?["\']([^"\']{10,}[^"\']*)["\']'
            ],
            'missing_localization': [
                r't\([^)]*["\']([^"\']+\.[^"\']+)["\']',  # –í–∏–∫–ª–∏–∫–∏ –ª–æ–∫–∞–ª—ñ–∑–∞—Ü—ñ—ó
                r't_sync\([^)]*["\']([^"\']+\.[^"\']+)["\']'
            ]
        }

        # –í—ñ–¥–æ–º—ñ –∫–æ–º–∞–Ω–¥–∏, —è–∫—ñ –º–∞—é—Ç—å –±—É—Ç–∏ —Ä–µ–∞–ª—ñ–∑–æ–≤–∞–Ω—ñ (–∑ help —Ç–∞ —ñ–Ω—Ç–µ—Ä—Ñ–µ–π—Å—É)
        self.advertised_commands = {
            'start', 'help', 'new', 'continue', 'ls', 'cd', 'pwd', 'projects',
            'status', 'export', 'actions', 'git', 'schedules', 'add_schedule',
            'settings', 'history', 'debug', 'explain'
        }

        # –ö–µ—à AST –¥–ª—è —Ñ–∞–π–ª—ñ–≤
        self.ast_cache = {}
        self.function_locations = {}  # –ó–±–µ—Ä—ñ–≥–∞—î –º—ñ—Å—Ü–µ–∑–Ω–∞—Ö–æ–¥–∂–µ–Ω–Ω—è —Ñ—É–Ω–∫—Ü—ñ–π

        # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –ø–µ—Ä–µ–∫–ª–∞–¥–∏ –ø—Ä–∏ —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—ó
        self.load_translations()

    def load_translations(self):
        """–ó–∞–≤–∞–Ω—Ç–∞–∂—É—î —Ñ–∞–π–ª–∏ –ø–µ—Ä–µ–∫–ª–∞–¥—ñ–≤ —Ç–∞ –∑–±–∏—Ä–∞—î –≤—Å—ñ –∫–ª—é—á—ñ."""
        for lang, path in self.translation_files.items():
            try:
                with open(path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.translations[lang] = data
                    self.translation_keys[lang] = self._extract_all_keys(data)
                    logger.info(f"–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {lang} –ø–µ—Ä–µ–∫–ª–∞–¥–∏ –∑ {path}")
            except Exception as e:
                logger.error(f"–ù–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ {lang} –ø–µ—Ä–µ–∫–ª–∞–¥–∏: {e}")
                self.translations[lang] = {}
                self.translation_keys[lang] = set()

    def _extract_all_keys(self, data: Any, prefix: str = "") -> Set[str]:
        """–†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –≤–∏—Ç—è–≥—É—î –≤—Å—ñ –∫–ª—é—á—ñ –∑ JSON-—Å—Ç—Ä—É–∫—Ç—É—Ä–∏."""
        keys = set()
        if isinstance(data, dict):
            for key, value in data.items():
                full_key = f"{prefix}.{key}" if prefix else key
                keys.add(full_key)
                keys.update(self._extract_all_keys(value, full_key))
        return keys

    def scan_all_files(self):
        """–°–∫–∞–Ω—É—î –≤—Å—ñ Python-—Ñ–∞–π–ª–∏ –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—ó —Ç–∞ –∑–∞–ø—É—Å–∫–∞—î –º–æ–¥—É–ª—ñ –∞—É–¥–∏—Ç—É."""
        logger.info("üîç –ü–æ—á–∞—Ç–æ–∫ –ø–æ–≤–Ω–æ–≥–æ –∞—É–¥–∏—Ç—É...")
        python_files = list(self.source_dir.rglob("*.py"))
        
        total_files = len(python_files)
        logger.info(f"–ó–Ω–∞–π–¥–µ–Ω–æ {total_files} Python-—Ñ–∞–π–ª—ñ–≤ –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É")
        
        for i, file_path in enumerate(python_files, 1):
            logger.info(f"–ê–Ω–∞–ª—ñ–∑ —Ñ–∞–π–ª—É {i}/{total_files}: {file_path}")
            try:
                self.analyze_file(file_path)
            except Exception as e:
                logger.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª—ñ–∑—ñ {file_path}: {e}")

        # –î–æ–¥–∞—Ç–∫–æ–≤—ñ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏
        self.check_advertised_commands()
        self.validate_localization_keys()
        self.analyze_user_journeys()
        self.test_integration_points()
        
        logger.info("‚úÖ –ü–æ–≤–Ω–∏–π –∞—É–¥–∏—Ç –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")

    def analyze_file(self, file_path: Path):
        """–ê–Ω–∞–ª—ñ–∑—É—î –æ–∫—Ä–µ–º–∏–π —Ñ–∞–π–ª –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é AST —Ç–∞ —Ä–µ–≥—É–ª—è—Ä–Ω–∏—Ö –≤–∏—Ä–∞–∑—ñ–≤."""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                source_code = f.read()
                tree = ast.parse(source_code)
                self.ast_cache[file_path] = tree
                
                # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –º—ñ—Å—Ü–µ–∑–Ω–∞—Ö–æ–¥–∂–µ–Ω–Ω—è —Ñ—É–Ω–∫—Ü—ñ–π –¥–ª—è –ø–æ–¥–∞–ª—å—à–æ–≥–æ –∞–Ω–∞–ª—ñ–∑—É
                self._extract_function_locations(file_path, tree)
                
        except Exception as e:
            logger.error(f"–ù–µ –≤–¥–∞–ª–æ—Å—è —Ä–æ–∑—ñ–±—Ä–∞—Ç–∏ AST –¥–ª—è {file_path}: {e}")
            return

        # 1. –ü–æ—à—É–∫ –∫—Ä–∏—Ç–∏—á–Ω–∏—Ö –ø—Ä–æ–±–ª–µ–º
        self._find_critical_issues(file_path, source_code)
        
        # 2. –ü–æ—à—É–∫ –ø—Ä–æ–±–ª–µ–º –ª–æ–∫–∞–ª—ñ–∑–∞—Ü—ñ—ó —Ç–∞ UX
        self._find_localization_and_ux_issues(file_path, source_code)
        
        # 3. –ê–Ω–∞–ª—ñ–∑ –∫–Ω–æ–ø–æ–∫
        self._analyze_buttons(file_path, source_code)

    def _extract_function_locations(self, file_path: Path, tree: ast.AST):
        """–í–∏—Ç—è–≥—É—î –º—ñ—Å—Ü–µ–∑–Ω–∞—Ö–æ–¥–∂–µ–Ω–Ω—è —Ñ—É–Ω–∫—Ü—ñ–π –∑ AST –¥–ª—è –ø–æ–¥–∞–ª—å—à–æ–≥–æ –∞–Ω–∞–ª—ñ–∑—É."""
        for node in ast.walk(tree):
            if isinstance(node, ast.FunctionDef):
                func_name = node.name
                if func_name not in self.function_locations:
                    self.function_locations[func_name] = []
                self.function_locations[func_name].append({
                    'file': str(file_path),
                    'line': node.lineno,
                    'end_line': getattr(node, 'end_lineno', node.lineno)
                })

    def _find_critical_issues(self, file_path: Path, source_code: str):
        """–®—É–∫–∞—î –∫—Ä–∏—Ç–∏—á–Ω—ñ –ø—Ä–æ–±–ª–µ–º–∏: –º–µ—Ä—Ç–≤—ñ –∫–æ–º–∞–Ω–¥–∏, —Ç–∏—Ö—ñ –∑–±–æ—ó, –ø–æ–º–∏–ª–∫–∏ –¥–ª—è –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞."""
        lines = source_code.split('\n')
        
        for pattern_name, patterns in self.CRITICAL_PATTERNS.items():
            for pattern in patterns:
                for match in re.finditer(pattern, source_code, re.DOTALL):
                    line_num = source_code[:match.start()].count('\n') + 1
                    line_content = lines[line_num - 1] if line_num <= len(lines) else ""
                    
                    issue = {
                        'file': str(file_path),
                        'line': line_num,
                        'pattern_type': pattern_name,
                        'match': match.group(0),
                        'line_content': line_content,
                        'command_or_button': match.group(1) if len(match.groups()) > 0 else None
                    }
                    
                    # –î–æ–¥–∞—Ç–∫–æ–≤–∏–π –∞–Ω–∞–ª—ñ–∑ –¥–ª—è –º–µ—Ä—Ç–≤–∏—Ö –∫–æ–º–∞–Ω–¥
                    if pattern_name == 'dead_commands' and issue['command_or_button']:
                        command = issue['command_or_button']
                        if command in self.advertised_commands:
                            issue['severity'] = 'critical'
                            issue['description'] = f"–ö–æ–º–∞–Ω–¥–∞ /{command} –æ–≥–æ–ª–æ—à–µ–Ω–∞, –∞–ª–µ –Ω–µ —Ä–µ–∞–ª—ñ–∑–æ–≤–∞–Ω–∞ –∞–±–æ –º—ñ—Å—Ç–∏—Ç—å NotImplementedError"
                    
                    self.findings['critical'].append(issue)
                    logger.warning(f"–ö—Ä–∏—Ç–∏—á–Ω–∞ –ø—Ä–æ–±–ª–µ–º–∞ —É {file_path}:{line_num} - {pattern_name}")

    def _find_localization_and_ux_issues(self, file_path: Path, source_code: str):
        """–®—É–∫–∞—î –ø—Ä–æ–±–ª–µ–º–∏ –∑ –ª–æ–∫–∞–ª—ñ–∑–∞—Ü—ñ—î—é —Ç–∞ UX: –∑–º—ñ—à–∞–Ω—ñ –º–æ–≤–∏, –∂–æ—Ä—Å—Ç–∫–æ –∑–∞–∫–æ–¥–æ–≤–∞–Ω—ñ —Ä—è–¥–∫–∏."""
        lines = source_code.split('\n')
        
        # –ü–æ—à—É–∫ –∑–º—ñ—à–∞–Ω–∏—Ö –º–æ–≤
        for pattern in self.UX_PATTERNS['mixed_languages']:
            for match in re.finditer(pattern, source_code):
                line_num = source_code[:match.start()].count('\n') + 1
                line_content = lines[line_num - 1] if line_num <= len(lines) else ""
                
                issue = {
                    'file': str(file_path),
                    'line': line_num,
                    'type': 'mixed_languages',
                    'snippet': match.group(0),
                    'line_content': line_content,
                    'severity': 'high'
                }
                self.findings['localization'].append(issue)
                logger.info(f"–ó–º—ñ—à–∞–Ω–∞ –º–æ–≤–∞ —É {file_path}:{line_num}")

        # –ü–æ—à—É–∫ –∂–æ—Ä—Å—Ç–∫–æ –∑–∞–∫–æ–¥–æ–≤–∞–Ω–∏—Ö —Ä—è–¥–∫—ñ–≤
        for pattern in self.UX_PATTERNS['hardcoded_strings']:
            for match in re.finditer(pattern, source_code):
                text = match.group(1)
                line_num = source_code[:match.start()].count('\n') + 1
                line_content = lines[line_num - 1] if line_num <= len(lines) else ""
                
                # –Ü–≥–Ω–æ—Ä—É—î–º–æ —Ä—è–¥–∫–∏, —è–∫—ñ –≤–∏–≥–ª—è–¥–∞—é—Ç—å —è–∫ —à–ª—è—Ö–∏, –∑–º—ñ–Ω–Ω—ñ –∞–±–æ —Ç–µ—Ö–Ω—ñ—á–Ω—ñ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è
                if any(ignore in text for ignore in ['{', '}', '%s', '%d', 'http', '.py', '__', '://', 'API', 'ID', 'token']):
                    continue
                
                # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ —Ü–µ –Ω–µ –∫–ª—é—á –ø–µ—Ä–µ–∫–ª–∞–¥—É (–Ω–µ –º—ñ—Å—Ç–∏—Ç—å –∫—Ä–∞–ø–æ–∫ –∞–±–æ –º–∞—î –ø—Ä–æ–±—ñ–ª–∏)
                if '.' not in text and ' ' in text and len(text) > 5:
                    issue = {
                        'file': str(file_path),
                        'line': line_num,
                        'type': 'hardcoded_string',
                        'text': text,
                        'line_content': line_content,
                        'severity': 'high'
                    }
                    self.findings['localization'].append(issue)
                    logger.info(f"–ñ–æ—Ä—Å—Ç–∫–æ –∑–∞–∫–æ–¥–æ–≤–∞–Ω–∏–π —Ä—è–¥–æ–∫ —É {file_path}:{line_num} - '{text}'")

        # –ü–æ—à—É–∫ –≤—ñ–¥—Å—É—Ç–Ω—ñ—Ö –ø–µ—Ä–µ–∫–ª–∞–¥—ñ–≤
        for pattern in self.UX_PATTERNS['missing_localization']:
            for match in re.finditer(pattern, source_code):
                key = match.group(1)
                line_num = source_code[:match.start()].count('\n') + 1
                line_content = lines[line_num - 1] if line_num <= len(lines) else ""
                
                # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ –∫–ª—é—á —ñ—Å–Ω—É—î –≤ –æ–±–æ—Ö –º–æ–≤–∞—Ö
                if key not in self.translation_keys['en']:
                    issue = {
                        'file': str(file_path),
                        'line': line_num,
                        'type': 'missing_translation',
                        'key': key,
                        'missing_in': 'en',
                        'line_content': line_content,
                        'severity': 'medium'
                    }
                    self.findings['localization'].append(issue)
                    logger.warning(f"–ö–ª—é—á –ø–µ—Ä–µ–∫–ª–∞–¥—É {key} –≤—ñ–¥—Å—É—Ç–Ω—ñ–π –≤ en.json")
                
                if key not in self.translation_keys['uk']:
                    issue = {
                        'file': str(file_path),
                        'line': line_num,
                        'type': 'missing_translation',
                        'key': key,
                        'missing_in': 'uk',
                        'line_content': line_content,
                        'severity': 'critical'  # –î–ª—è —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—ó –º–æ–≤–∏ —Ü–µ –∫—Ä–∏—Ç–∏—á–Ω–æ
                    }
                    self.findings['localization'].append(issue)
                    logger.warning(f"–ö–ª—é—á –ø–µ—Ä–µ–∫–ª–∞–¥—É {key} –≤—ñ–¥—Å—É—Ç–Ω—ñ–π –≤ uk.json")

    def _analyze_buttons(self, file_path: Path, source_code: str):
        """–ê–Ω–∞–ª—ñ–∑—É—î –∫–Ω–æ–ø–∫–∏ —Ç–∞ —ó—Ö–Ω—ñ callback_data."""
        pattern = r'InlineKeyboardButton\(["\']([^"\']+)["\'].*?callback_data=["\']([^"\']+)["\']'
        lines = source_code.split('\n')
        
        for match in re.finditer(pattern, source_code):
            button_text = match.group(1)
            callback_data = match.group(2)
            line_num = source_code[:match.start()].count('\n') + 1
            line_content = lines[line_num - 1] if line_num <= len(lines) else ""

            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ —ñ—Å–Ω—É—î –æ–±—Ä–æ–±–Ω–∏–∫ –¥–ª—è —Ü—å–æ–≥–æ callback_data
            handler_exists = False
            
            # –®—É–∫–∞—î–º–æ –æ–±—Ä–æ–±–Ω–∏–∫ –≤ AST
            if file_path in self.ast_cache:
                tree = self.ast_cache[file_path]
                for node in ast.walk(tree):
                    if isinstance(node, ast.FunctionDef):
                        # –®—É–∫–∞—î–º–æ –≤–∏–∫–ª–∏–∫–∏ register_callback –∞–±–æ –ø–æ–¥—ñ–±–Ω—ñ
                        for child in ast.walk(node):
                            if isinstance(child, ast.Call) and isinstance(child.func, ast.Name):
                                if child.func.id in ['register_callback', 'add_handler', 'CommandHandler']:
                                    if len(child.args) > 0 and isinstance(child.args[0], ast.Str):
                                        if child.args[0].s == callback_data:
                                            handler_exists = True
                                            break
                            elif isinstance(child, ast.Assign):
                                # –®—É–∫–∞—î–º–æ —Å–ª–æ–≤–Ω–∏–∫–∏ –∑ callback_data
                                if isinstance(child.value, ast.Dict):
                                    for key, value in zip(child.value.keys, child.value.values):
                                        if isinstance(key, ast.Str) and key.s == callback_data:
                                            handler_exists = True
                                            break
                    
                    if handler_exists:
                        break
            
            # –¢–∞–∫–æ–∂ –ø–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –∑–∞ —ñ–º–µ–Ω–µ–º —Ñ—É–Ω–∫—Ü—ñ—ó
            if not handler_exists:
                # –°–ø—Ä–æ–±—É—î–º–æ –∑–Ω–∞–π—Ç–∏ —Ñ—É–Ω–∫—Ü—ñ—é –∑ —ñ–º–µ–Ω–µ–º, —â–æ –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—î callback_data
                possible_function_names = [
                    f"{callback_data}_callback",
                    f"handle_{callback_data}",
                    callback_data
                ]
                
                for func_name in possible_function_names:
                    if func_name in self.function_locations:
                        handler_exists = True
                        break
            
            issue = {
                'file': str(file_path),
                'line': line_num,
                'button_text': button_text,
                'callback_data': callback_data,
                'handler_exists': handler_exists,
                'line_content': line_content,
                'severity': 'critical' if not handler_exists else 'info'
            }
            self.findings['buttons'].append(issue)
            
            if not handler_exists:
                logger.error(f"–ö–Ω–æ–ø–∫–∞ '{button_text}' (callback: {callback_data}) –Ω–µ –º–∞—î –æ–±—Ä–æ–±–Ω–∏–∫–∞ —É {file_path}:{line_num}")

    def check_advertised_commands(self):
        """–ü–µ—Ä–µ–≤—ñ—Ä—è—î, —á–∏ –≤—Å—ñ –æ–≥–æ–ª–æ—à–µ–Ω—ñ –∫–æ–º–∞–Ω–¥–∏ –º–∞—é—Ç—å —Ä–µ–∞–ª—ñ–∑–∞—Ü—ñ—é."""
        logger.info("üîç –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –æ–≥–æ–ª–æ—à–µ–Ω–∏—Ö –∫–æ–º–∞–Ω–¥...")
        
        # –ó–Ω–∞—Ö–æ–¥–∏–º–æ –≤—Å—ñ –∑–∞—Ä–µ—î—Å—Ç—Ä–æ–≤–∞–Ω—ñ –∫–æ–º–∞–Ω–¥–∏ —É –∫–æ–¥—ñ
        implemented_commands = set()
        python_files = list(self.source_dir.rglob("*.py"))
        
        command_pattern = r'CommandHandler\(["\'](\w+)["\']'
        for file_path in python_files:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                    for match in re.finditer(command_pattern, content):
                        implemented_commands.add(match.group(1))
            except Exception:
                continue

        # –ü–æ—Ä—ñ–≤–Ω—é—î–º–æ –∑ –æ–≥–æ–ª–æ—à–µ–Ω–∏–º–∏ –∫–æ–º–∞–Ω–¥–∞–º–∏
        for cmd in self.advertised_commands:
            if cmd not in implemented_commands:
                issue = {
                    'command': cmd,
                    'status': 'not_implemented',
                    'description': f"–ö–æ–º–∞–Ω–¥–∞ /{cmd} –æ–≥–æ–ª–æ—à–µ–Ω–∞ –≤ —ñ–Ω—Ç–µ—Ä—Ñ–µ–π—Å—ñ, –∞–ª–µ –Ω–µ –º–∞—î –æ–±—Ä–æ–±–Ω–∏–∫–∞",
                    'severity': 'critical'
                }
                self.findings['critical'].append(issue)
                logger.error(f"‚ùó –ö—Ä–∏—Ç–∏—á–Ω–æ: –ö–æ–º–∞–Ω–¥–∞ /{cmd} –Ω–µ —Ä–µ–∞–ª—ñ–∑–æ–≤–∞–Ω–∞!")

    def validate_localization_keys(self):
        """–ü–µ—Ä–µ–≤—ñ—Ä—è—î, —á–∏ –≤—Å—ñ –∫–ª—é—á—ñ –ø–µ—Ä–µ–∫–ª–∞–¥—É –ø—Ä–∏—Å—É—Ç–Ω—ñ –≤ –æ–±–æ—Ö –º–æ–≤–∞—Ö."""
        logger.info("üåç –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –ø–æ–≤–Ω–æ—Ç–∏ –ø–µ—Ä–µ–∫–ª–∞–¥—ñ–≤...")
        
        missing_in_uk = self.translation_keys['en'] - self.translation_keys['uk']
        missing_in_en = self.translation_keys['uk'] - self.translation_keys['en']

        for key in missing_in_uk:
            issue = {
                'key': key,
                'missing_in': 'uk',
                'type': 'missing_translation',
                'severity': 'critical'  # –î–ª—è —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—ó –º–æ–≤–∏ —Ü–µ –∫—Ä–∏—Ç–∏—á–Ω–æ
            }
            self.findings['localization'].append(issue)
            logger.error(f"‚ùó –ö—Ä–∏—Ç–∏—á–Ω–æ: –í—ñ–¥—Å—É—Ç–Ω—ñ–π —É–∫—Ä–∞—ó–Ω—Å—å–∫–∏–π –ø–µ—Ä–µ–∫–ª–∞–¥ –¥–ª—è –∫–ª—é—á–∞ '{key}'")

        for key in missing_in_en:
            issue = {
                'key': key,
                'missing_in': 'en',
                'type': 'missing_translation',
                'severity': 'medium'
            }
            self.findings['localization'].append(issue)
            logger.warning(f"–ü–æ–ø–µ—Ä–µ–¥–∂–µ–Ω–Ω—è: –í—ñ–¥—Å—É—Ç–Ω—ñ–π –∞–Ω–≥–ª—ñ–π—Å—å–∫–∏–π –ø–µ—Ä–µ–∫–ª–∞–¥ –¥–ª—è –∫–ª—é—á–∞ '{key}'")

    def analyze_user_journeys(self):
        """–ê–Ω–∞–ª—ñ–∑—É—î –ø–æ–≤–Ω—ñ —à–ª—è—Ö–∏ –≤–∑–∞—î–º–æ–¥—ñ—ó –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞."""
        logger.info("üó∫Ô∏è –ê–Ω–∞–ª—ñ–∑ —à–ª—è—Ö—ñ–≤ –≤–∑–∞—î–º–æ–¥—ñ—ó –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞...")
        
        # –í–∏–∑–Ω–∞—á–∞—î–º–æ –æ—Å–Ω–æ–≤–Ω—ñ —à–ª—è—Ö–∏ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞
        user_journeys = {
            'start_new_session': ['/start', '/new', '/ls', '/cd', '/help'],
            'quick_actions': ['/actions', 'continue', 'export_session', 'save_code'],
            'project_management': ['/projects', '/git', '/schedules'],
            'settings': ['/settings', 'lang:select', 'toggle_language']
        }
        
        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –∫–æ–∂–µ–Ω —à–ª—è—Ö
        for journey_name, commands in user_journeys.items():
            journey_issues = []
            
            for cmd in commands:
                if cmd.startswith('/'):
                    # –¶–µ –∫–æ–º–∞–Ω–¥–∞
                    if not any(issue.get('command') == cmd[1:] for issue in self.findings['critical'] if issue.get('status') == 'not_implemented'):
                        # –ö–æ–º–∞–Ω–¥–∞ —Ä–µ–∞–ª—ñ–∑–æ–≤–∞–Ω–∞
                        pass
                    else:
                        journey_issues.append(f"–ö–æ–º–∞–Ω–¥–∞ {cmd} –Ω–µ —Ä–µ–∞–ª—ñ–∑–æ–≤–∞–Ω–∞")
                else:
                    # –¶–µ callback
                    if not any(btn.get('callback_data') == cmd and btn.get('handler_exists') for btn in self.findings['buttons']):
                        journey_issues.append(f"Callback {cmd} –Ω–µ –º–∞—î –æ–±—Ä–æ–±–Ω–∏–∫–∞")
            
            if journey_issues:
                issue = {
                    'journey': journey_name,
                    'issues': journey_issues,
                    'type': 'broken_user_journey',
                    'severity': 'high'
                }
                self.findings['ux'].append(issue)
                logger.warning(f"–ó–ª–∞–º–∞–Ω–∏–π —à–ª—è—Ö –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞ '{journey_name}': {', '.join(journey_issues)}")

    def test_integration_points(self):
        """–¢–µ—Å—Ç—É—î —Ç–æ—á–∫–∏ —ñ–Ω—Ç–µ–≥—Ä–∞—Ü—ñ—ó –∑–æ–≤–Ω—ñ—à–Ω—ñ—Ö —Å–∏—Å—Ç–µ–º."""
        logger.info("üîå –¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è —Ç–æ—á–æ–∫ —ñ–Ω—Ç–µ–≥—Ä–∞—Ü—ñ—ó...")
        
        integration_patterns = {
            'claude_cli': [
                r'claude\s+ask',
                r'claude\s+--version',
                r'from\s+...claude\s+import',
                r'ClaudeIntegration',
                r'ClaudeProcessManager'
            ],
            'file_system': [
                r'os\.(listdir|chdir|getcwd|path)',
                r'shutil\.',
                r'open\(',
                r'with\s+open\('
            ],
            'database': [
                r'import\s+sqlite3',
                r'from\s+aiosqlite',
                r'SessionManager',
                r'StorageManager'
            ],
            'docker': [
                r'docker\s+exec',
                r'docker\s+run',
                r'container',
                r'Dockerfile'
            ]
        }
        
        python_files = list(self.source_dir.rglob("*.py"))
        
        for integration_type, patterns in integration_patterns.items():
            for file_path in python_files:
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                        
                    for pattern in patterns:
                        for match in re.finditer(pattern, content):
                            line_num = content[:match.start()].count('\n') + 1
                            
                            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ —î –Ω–∞–ª–µ–∂–Ω–∞ –æ–±—Ä–æ–±–∫–∞ –ø–æ–º–∏–ª–æ–∫
                            has_error_handling = False
                            
                            # –®—É–∫–∞—î–º–æ try-except –±–ª–æ–∫–∏ –Ω–∞–≤–∫–æ–ª–æ —Ü—å–æ–≥–æ —Ä—è–¥–∫–∞
                            lines = content.split('\n')
                            start_line = max(0, line_num - 5)
                            end_line = min(len(lines), line_num + 5)
                            
                            context = "\n".join(lines[start_line:end_line])
                            if 'try:' in context and ('except' in context or 'finally' in context):
                                has_error_handling = True
                            
                            if not has_error_handling:
                                issue = {
                                    'file': str(file_path),
                                    'line': line_num,
                                    'integration_type': integration_type,
                                    'pattern': pattern,
                                    'match': match.group(0),
                                    'type': 'integration_without_error_handling',
                                    'severity': 'high',
                                    'description': f"–¢–æ—á–∫–∞ —ñ–Ω—Ç–µ–≥—Ä–∞—Ü—ñ—ó '{integration_type}' –±–µ–∑ –Ω–∞–ª–µ–∂–Ω–æ—ó –æ–±—Ä–æ–±–∫–∏ –ø–æ–º–∏–ª–æ–∫"
                                }
                                self.findings['integration'].append(issue)
                                logger.warning(f"–Ü–Ω—Ç–µ–≥—Ä–∞—Ü—ñ—è –±–µ–∑ –æ–±—Ä–æ–±–∫–∏ –ø–æ–º–∏–ª–æ–∫: {integration_type} —É {file_path}:{line_num}")
                                
                except Exception as e:
                    logger.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª—ñ–∑—ñ —ñ–Ω—Ç–µ–≥—Ä–∞—Ü—ñ—ó —É {file_path}: {e}")

    def generate_report(self) -> str:
        """–ì–µ–Ω–µ—Ä—É—î –∑–≤—ñ—Ç –ø—Ä–æ –∑–Ω–∞—Ö—ñ–¥–∫–∏ —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é –º–æ–≤–æ—é."""
        report_lines = []
        report_lines.append("# üéØ –†–û–ó–®–ò–†–ï–ù–ò–ô –ê–£–î–ò–¢ –î–û–°–í–Ü–î–£ –ö–û–†–ò–°–¢–£–í–ê–ß–ê\n")
        report_lines.append(f"**–ó–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–æ:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        report_lines.append(f"**–ü—Ä–æ–∞–Ω–∞–ª—ñ–∑–æ–≤–∞–Ω–æ —Ñ–∞–π–ª—ñ–≤:** {len(self.ast_cache)}\n\n")

        total_issues = sum(len(v) for v in self.findings.values())
        critical_issues = len([i for i in self.findings['critical'] + self.findings['localization'] + self.findings['buttons'] if i.get('severity') == 'critical'])
        high_issues = len([i for i in self.findings['critical'] + self.findings['localization'] + self.findings['ux'] + self.findings['integration'] if i.get('severity') == 'high'])
        medium_issues = len([i for i in self.findings['localization'] if i.get('severity') == 'medium'])
        
        report_lines.append("## üìä –ó–ê–ì–ê–õ–¨–ù–ò–ô –ó–í–Ü–¢\n")
        report_lines.append(f"- **–í—Å—å–æ–≥–æ –ø—Ä–æ–±–ª–µ–º –∑–Ω–∞–π–¥–µ–Ω–æ:** {total_issues}\n")
        report_lines.append(f"- **üî¥ –ö—Ä–∏—Ç–∏—á–Ω–∏—Ö (–ø–æ—Ç—Ä—ñ–±–Ω–æ –≤–∏–ø—Ä–∞–≤–∏—Ç–∏ –Ω–µ–≥–∞–π–Ω–æ):** {critical_issues}\n")
        report_lines.append(f"- **üü† –í–∏—Å–æ–∫–æ–≥–æ –ø—Ä—ñ–æ—Ä–∏—Ç–µ—Ç—É (–ø–æ—Ç—Ä—ñ–±–Ω–æ –≤–∏–ø—Ä–∞–≤–∏—Ç–∏ —Ü—å–æ–≥–æ —Ç–∏–∂–Ω—è):** {high_issues}\n")
        report_lines.append(f"- **üü° –°–µ—Ä–µ–¥–Ω—å–æ–≥–æ –ø—Ä—ñ–æ—Ä–∏—Ç–µ—Ç—É (–ø–æ–ª—ñ–ø—à–µ–Ω–Ω—è —ñ–Ω—Ç–µ—Ä—Ñ–µ–π—Å—É):** {medium_issues}\n\n")

        # –ö—Ä–∏—Ç–∏—á–Ω—ñ –ø—Ä–æ–±–ª–µ–º–∏
        critical_findings = [i for i in self.findings['critical'] + self.findings['localization'] + self.findings['buttons'] if i.get('severity') == 'critical']
        if len(critical_findings) > 0:
            report_lines.append("## üî¥ –ö–†–ò–¢–ò–ß–ù–Ü –ü–†–û–ë–õ–ï–ú–ò (–í–ò–ü–†–ê–í–ò–¢–ò –ù–ï–ì–ê–ô–ù–û)\n")
            for i, issue in enumerate(critical_findings, 1):
                if 'command' in issue:
                    report_lines.append(f"### C{i}: –ù–ï–ü–†–ê–¶–Æ–Æ–ß–ê –ö–û–ú–ê–ù–î–ê\n")
                    report_lines.append(f"**–ü—Ä–æ–±–ª–µ–º–∞:** –ö–æ–º–∞–Ω–¥–∞ `/{issue['command']}` –æ–≥–æ–ª–æ—à–µ–Ω–∞, –∞–ª–µ –Ω–µ —Ä–µ–∞–ª—ñ–∑–æ–≤–∞–Ω–∞\n")
                    report_lines.append(f"**–©–æ –±–∞—á–∏—Ç—å –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á:** –ù–∞–±–∏—Ä–∞—î `/{issue['command']}` ‚Üí –æ—Ç—Ä–∏–º—É—î –ø–æ–º–∏–ª–∫—É –∞–±–æ –Ω—ñ—á–æ–≥–æ\n")
                    report_lines.append(f"**–†—ñ—à–µ–Ω–Ω—è:** –†–µ–∞–ª—ñ–∑—É–≤–∞—Ç–∏ –æ–±—Ä–æ–±–Ω–∏–∫ –∫–æ–º–∞–Ω–¥–∏ –∞–±–æ –ø—Ä–∏–±—Ä–∞—Ç–∏ –∑ –¥–æ–≤—ñ–¥–∫–∏/–º–µ–Ω—é\n\n")
                elif issue.get('type') == 'missing_translation' and issue.get('missing_in') == 'uk':
                    report_lines.append(f"### C{i}: –í–Ü–î–°–£–¢–ù–Ü–ô –£–ö–†–ê–á–ù–°–¨–ö–ò–ô –ü–ï–†–ï–ö–õ–ê–î\n")
                    report_lines.append(f"**–ö–ª—é—á:** `{issue['key']}`\n")
                    report_lines.append(f"**–©–æ –±–∞—á–∏—Ç—å –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á:** –ó–∞–º—ñ—Å—Ç—å —Ç–µ–∫—Å—Ç—É –º–æ–∂–µ –ø–æ–∫–∞–∑—É–≤–∞—Ç–∏—Å—è –∫–ª—é—á –ø–µ—Ä–µ–∫–ª–∞–¥—É –∞–±–æ –∞–Ω–≥–ª—ñ–π—Å—å–∫–∏–π —Ç–µ–∫—Å—Ç\n")
                    report_lines.append(f"**–†—ñ—à–µ–Ω–Ω—è:** –î–æ–¥–∞—Ç–∏ –ø–µ—Ä–µ–∫–ª–∞–¥ —É `uk.json` —Ñ–∞–π–ª\n\n")
                elif 'callback_data' in issue and not issue.get('handler_exists', True):
                    report_lines.append(f"### C{i}: –ù–ï–ü–†–ê–¶–Æ–Æ–ß–ê –ö–ù–û–ü–ö–ê\n")
                    report_lines.append(f"**–¢–µ–∫—Å—Ç –∫–Ω–æ–ø–∫–∏:** `{issue['button_text']}`\n")
                    report_lines.append(f"**Callback:** `{issue['callback_data']}`\n")
                    report_lines.append(f"**–§–∞–π–ª:** `{issue['file']}` (—Ä—è–¥–æ–∫ {issue['line']})\n")
                    report_lines.append(f"**–©–æ –±–∞—á–∏—Ç—å –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á:** –ù–∞—Ç–∏—Å–∫–∞—î –∫–Ω–æ–ø–∫—É ‚Üí –Ω—ñ—á–æ–≥–æ –Ω–µ –≤—ñ–¥–±—É–≤–∞—î—Ç—å—Å—è –∞–±–æ –ø–æ–º–∏–ª–∫–∞\n")
                    report_lines.append(f"**–†—ñ—à–µ–Ω–Ω—è:** –†–µ–∞–ª—ñ–∑—É–≤–∞—Ç–∏ –æ–±—Ä–æ–±–Ω–∏–∫ –∞–±–æ –ø—Ä–∏–±—Ä–∞—Ç–∏ –∫–Ω–æ–ø–∫—É\n\n")
                else:
                    report_lines.append(f"### C{i}: {issue.get('pattern_type', '–ù–µ–≤—ñ–¥–æ–º–∞ –ø—Ä–æ–±–ª–µ–º–∞')}\n")
                    report_lines.append(f"**–§–∞–π–ª:** `{issue['file']}` (—Ä—è–¥–æ–∫ {issue['line']})\n")
                    report_lines.append(f"**–ö–æ–¥:** `{issue.get('match', issue.get('line_content', ''))}`\n")
                    report_lines.append(f"**–†—ñ—à–µ–Ω–Ω—è:** –ü–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ –ª–æ–≥—ñ–∫—É –æ–±—Ä–æ–±–∫–∏ —Ç–∞ –¥–æ–¥–∞—Ç–∏ –∫–æ—Ä–µ–∫—Ç–Ω—É –≤—ñ–¥–ø–æ–≤—ñ–¥—å –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—É\n\n")

        # –ü—Ä–æ–±–ª–µ–º–∏ –≤–∏—Å–æ–∫–æ–≥–æ –ø—Ä—ñ–æ—Ä–∏—Ç–µ—Ç—É
        high_findings = [i for i in self.findings['critical'] + self.findings['localization'] + self.findings['ux'] + self.findings['integration'] if i.get('severity') == 'high']
        if len(high_findings) > 0:
            report_lines.append("## üü† –ü–†–û–ë–õ–ï–ú–ò –í–ò–°–û–ö–û–ì–û –ü–†–Ü–û–†–ò–¢–ï–¢–£ (–í–ò–ü–†–ê–í–ò–¢–ò –¶–¨–û–ì–û –¢–ò–ñ–ù–Ø)\n")
            for i, issue in enumerate(high_findings, 1):
                if issue.get('type') == 'mixed_languages':
                    report_lines.append(f"### H{i}: –ó–ú–Ü–®–ê–ù–Ü –ú–û–í–ò\n")
                    report_lines.append(f"**–§–∞–π–ª:** `{issue['file']}` (—Ä—è–¥–æ–∫ {issue['line']})\n")
                    report_lines.append(f"**–§—Ä–∞–≥–º–µ–Ω—Ç:** `{issue['snippet']}`\n")
                    report_lines.append(f"**–©–æ –±–∞—á–∏—Ç—å –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á:** –£–∫—Ä–∞—ó–Ω—Å—å–∫–∏–π —ñ–Ω—Ç–µ—Ä—Ñ–µ–π—Å –∑ –∞–Ω–≥–ª—ñ–π—Å—å–∫–∏–º–∏ –ø–æ–º–∏–ª–∫–∞–º–∏\n")
                    report_lines.append(f"**–†—ñ—à–µ–Ω–Ω—è:** –ü–µ—Ä–µ–∫–ª–∞—Å—Ç–∏ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è –ø–æ–≤–Ω—ñ—Å—Ç—é —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é\n\n")
                elif issue.get('type') == 'hardcoded_string':
                    report_lines.append(f"### H{i}: –ñ–û–†–°–¢–ö–û –ó–ê–ö–û–î–û–í–ê–ù–ò–ô –†–Ø–î–û–ö\n")
                    report_lines.append(f"**–§–∞–π–ª:** `{issue['file']}` (—Ä—è–¥–æ–∫ {issue['line']})\n")
                    report_lines.append(f"**–¢–µ–∫—Å—Ç:** `{issue['text']}`\n")
                    report_lines.append(f"**–©–æ –±–∞—á–∏—Ç—å –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á:** –¢–µ–∫—Å—Ç, —è–∫–∏–π –Ω–µ –∑–º—ñ–Ω—é—î—Ç—å—Å—è –ø—Ä–∏ –∑–º—ñ–Ω—ñ –º–æ–≤–∏\n")
                    report_lines.append(f"**–†—ñ—à–µ–Ω–Ω—è:** –í–∏–Ω–µ—Å—Ç–∏ —Ç–µ–∫—Å—Ç —É —Ñ–∞–π–ª–∏ –ø–µ—Ä–µ–∫–ª–∞–¥—ñ–≤ —Ç–∞ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ —Ñ—É–Ω–∫—Ü—ñ—é –ª–æ–∫–∞–ª—ñ–∑–∞—Ü—ñ—ó\n\n")
                elif issue.get('type') == 'broken_user_journey':
                    report_lines.append(f"### H{i}: –ó–õ–ê–ú–ê–ù–ò–ô –®–õ–Ø–• –ö–û–†–ò–°–¢–£–í–ê–ß–ê\n")
                    report_lines.append(f"**–®–ª—è—Ö:** `{issue['journey']}`\n")
                    report_lines.append(f"**–ü—Ä–æ–±–ª–µ–º–∏:** {', '.join(issue['issues'])}\n")
                    report_lines.append(f"**–©–æ –±–∞—á–∏—Ç—å –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á:** –ù–µ –º–æ–∂–µ –∑–∞–≤–µ—Ä—à–∏—Ç–∏ –æ—á—ñ–∫—É–≤–∞–Ω—É –¥—ñ—é\n")
                    report_lines.append(f"**–†—ñ—à–µ–Ω–Ω—è:** –†–µ–∞–ª—ñ–∑—É–≤–∞—Ç–∏ –≤—ñ–¥—Å—É—Ç–Ω—ñ –∫–æ–º–∞–Ω–¥–∏ –∞–±–æ –æ–±—Ä–æ–±–Ω–∏–∫–∏ –∫–Ω–æ–ø–æ–∫\n\n")
                elif issue.get('type') == 'integration_without_error_handling':
                    report_lines.append(f"### H{i}: –Ü–ù–¢–ï–ì–†–ê–¶–Ü–Ø –ë–ï–ó –û–ë–†–û–ë–ö–ò –ü–û–ú–ò–õ–û–ö\n")
                    report_lines.append(f"**–¢–∏–ø —ñ–Ω—Ç–µ–≥—Ä–∞—Ü—ñ—ó:** `{issue['integration_type']}`\n")
                    report_lines.append(f"**–§–∞–π–ª:** `{issue['file']}` (—Ä—è–¥–æ–∫ {issue['line']})\n")
                    report_lines.append(f"**–©–æ –±–∞—á–∏—Ç—å –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á:** –¢–µ—Ö–Ω—ñ—á–Ω—ñ –ø–æ–º–∏–ª–∫–∏ –∑–∞–º—ñ—Å—Ç—å –∑—Ä–æ–∑—É–º—ñ–ª–∏—Ö –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω—å\n")
                    report_lines.append(f"**–†—ñ—à–µ–Ω–Ω—è:** –î–æ–¥–∞—Ç–∏ try-except –±–ª–æ–∫–∏ –∑ –ª–æ–∫–∞–ª—ñ–∑–æ–≤–∞–Ω–∏–º–∏ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è–º–∏ –ø—Ä–æ –ø–æ–º–∏–ª–∫–∏\n\n")

        # –ü—Ä–æ–±–ª–µ–º–∏ —Å–µ—Ä–µ–¥–Ω—å–æ–≥–æ –ø—Ä—ñ–æ—Ä–∏—Ç–µ—Ç—É
        medium_findings = [i for i in self.findings['localization'] if i.get('severity') == 'medium']
        if len(medium_findings) > 0:
            report_lines.append("## üü° –ü–†–û–ë–õ–ï–ú–ò –°–ï–†–ï–î–ù–¨–û–ì–û –ü–†–Ü–û–†–ò–¢–ï–¢–£ (–ü–û–õ–Ü–ü–®–ï–ù–ù–Ø –Ü–ù–¢–ï–†–§–ï–ô–°–£)\n")
            for i, issue in enumerate(medium_findings, 1):
                if issue.get('type') == 'missing_translation' and issue.get('missing_in') == 'en':
                    report_lines.append(f"### M{i}: –í–Ü–î–°–£–¢–ù–Ü–ô –ê–ù–ì–õ–Ü–ô–°–¨–ö–ò–ô –ü–ï–†–ï–ö–õ–ê–î\n")
                    report_lines.append(f"**–ö–ª—é—á:** `{issue['key']}`\n")
                    report_lines.append(f"**–†—ñ—à–µ–Ω–Ω—è:** –î–æ–¥–∞—Ç–∏ –ø–µ—Ä–µ–∫–ª–∞–¥ —É `en.json` —Ñ–∞–π–ª –¥–ª—è –ø—ñ–¥—Ç—Ä–∏–º–∫–∏ –∞–Ω–≥–ª–æ–º–æ–≤–Ω–∏—Ö –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤\n\n")

        if total_issues == 0:
            report_lines.append("## üéâ –í–Ü–¢–ê–Ñ–ú–û!\n")
            report_lines.append("–ö—Ä–∏—Ç–∏—á–Ω–∏—Ö –ø—Ä–æ–±–ª–µ–º –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ. –ë–æ—Ç –≥–æ—Ç–æ–≤–∏–π –¥–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è!\n")

        # –î–æ–¥–∞–º–æ –º–µ—Ç—Ä–∏–∫–∏ —è–∫–æ—Å—Ç—ñ
        report_lines.append("## üìà –ú–ï–¢–†–ò–ö–ò –Ø–ö–û–°–¢–Ü\n")
        metrics = self.get_quality_metrics()
        report_lines.append(f"- **–ü–æ–∫—Ä–∏—Ç—Ç—è —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é –ª–æ–∫–∞–ª—ñ–∑–∞—Ü—ñ—î—é:** {metrics['localization_coverage_uk']}\n")
        report_lines.append(f"- **–ö—Ä–∏—Ç–∏—á–Ω–∏—Ö –ø—Ä–æ–±–ª–µ–º:** {metrics['critical_issues_count']}\n")
        report_lines.append(f"- **–ñ–æ—Ä—Å—Ç–∫–æ –∑–∞–∫–æ–¥–æ–≤–∞–Ω–∏—Ö —Ä—è–¥–∫—ñ–≤:** {metrics['hardcoded_strings_count']}\n")
        report_lines.append(f"- **–í—ñ–¥—Å—É—Ç–Ω—ñ—Ö —É–∫—Ä–∞—ó–Ω—Å—å–∫–∏—Ö –ø–µ—Ä–µ–∫–ª–∞–¥—ñ–≤:** {metrics['missing_translations_uk']}\n")
        report_lines.append(f"- **–†–µ–∞–ª—ñ–∑–æ–≤–∞–Ω–∏—Ö –æ–≥–æ–ª–æ—à–µ–Ω–∏—Ö –∫–æ–º–∞–Ω–¥:** {metrics['advertised_commands_implemented']} –∑ {len(self.advertised_commands)}\n")

        return "\n".join(report_lines)

    def save_report(self, filename: str = "advanced_audit_report_ua.md"):
        """–ó–±–µ—Ä—ñ–≥–∞—î –∑–≤—ñ—Ç —É —Ñ–∞–π–ª."""
        report_content = self.generate_report()
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(report_content)
        logger.info(f"‚úÖ –ó–≤—ñ—Ç –∑–±–µ—Ä–µ–∂–µ–Ω–æ —É {filename}")

    def get_quality_metrics(self) -> Dict[str, Any]:
        """–ü–æ–≤–µ—Ä—Ç–∞—î –º–µ—Ç—Ä–∏–∫–∏ —è–∫–æ—Å—Ç—ñ."""
        total_keys = len(self.translation_keys['en'])
        uk_coverage = len(self.translation_keys['uk']) / total_keys if total_keys > 0 else 0

        return {
            'localization_coverage_uk': f"{uk_coverage:.1%}",
            'critical_issues_count': len([i for i in self.findings['critical'] + self.findings['localization'] + self.findings['buttons'] if i.get('severity') == 'critical']),
            'hardcoded_strings_count': len([i for i in self.findings['localization'] if i.get('type') == 'hardcoded_string']),
            'missing_translations_uk': len([i for i in self.findings['localization'] if i.get('missing_in') == 'uk']),
            'advertised_commands_implemented': len(self.advertised_commands) - len([i for i in self.findings['critical'] if i.get('status') == 'not_implemented'])
        }

    def run_full_audit(self):
        """–ó–∞–ø—É—Å–∫–∞—î –ø–æ–≤–Ω–∏–π –∞—É–¥–∏—Ç —ñ –∑–±–µ—Ä—ñ–≥–∞—î –∑–≤—ñ—Ç."""
        logger.info("üöÄ –ó–∞–ø—É—Å–∫ –ø–æ–≤–Ω–æ–≥–æ –∞—É–¥–∏—Ç—É...")
        self.scan_all_files()
        self.save_report()
        metrics = self.get_quality_metrics()
        logger.info("üìä –ú–µ—Ç—Ä–∏–∫–∏ —è–∫–æ—Å—Ç—ñ:")
        for key, value in metrics.items():
            logger.info(f"  {key}: {value}")
        logger.info("‚úÖ –ê—É–¥–∏—Ç –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø—ñ—à–Ω–æ!")

if __name__ == "__main__":
    # –°—Ç–≤–æ—Ä—é—î–º–æ –∞—É–¥–∏—Ç–æ—Ä–∞
    auditor = AdvancedBotAuditor(source_dir="src", report_lang="uk")
    
    # –ó–∞–ø—É—Å–∫–∞—î–º–æ –ø–æ–≤–Ω–∏–π –∞—É–¥–∏—Ç
    auditor.run_full_audit()
    
    print("\nüéâ –ê—É–¥–∏—Ç –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø—ñ—à–Ω–æ!")
    print("üìÑ –ó–≤—ñ—Ç –∑–±–µ—Ä–µ–∂–µ–Ω–æ —É —Ñ–∞–π–ª—ñ: advanced_audit_report_ua.md")
    print("üîç –ü–µ—Ä–µ–≥–ª—è–Ω—å—Ç–µ –∑–≤—ñ—Ç –¥–ª—è –≤–∏–ø—Ä–∞–≤–ª–µ–Ω–Ω—è –ø—Ä–æ–±–ª–µ–º —É –±–æ—Ç—ñ!")